# -*- coding: utf-8 -*-
"""BertVit_fusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ItC-VPTkHRYP_xmMowcgGQrNXG7mnV9A
"""

!pip uninstall -y torch torchvision torchaudio
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

"""**bold text**

**758** **samples**
"""

from google.colab import drive
drive.mount('/content/drive')

# Step 1: Install gdown if not already installed
!pip install -q gdown

# Step 2: Download the RCB.zip file correctly
!gdown --id 1IrrswV9bXz6jjz8SUEVfBY6ILRJ17-xy

# Step 3: Unzip it properly
import zipfile

with zipfile.ZipFile("RCB.zip", 'r') as zip_ref:
    zip_ref.extractall("rcb_dataset")  # Extract to "rcb_dataset" folder

print("âœ… Extraction completed!")

https://drive.google.com/file/d/13JFv1NjSMc1AajT3kxYA48UXm5KKQhfA/view?usp=sharing

# Step 3: Extract the ZIP contents
import zipfile

with zipfile.ZipFile("RCB.zip", 'r') as zip_ref:
    zip_ref.extractall("rcb_dataset")

import os

# Check top-level folder after extraction
print("Top-level folders:", os.listdir("rcb_dataset"))

# Check subfolders inside 'RCB'
print("Subfolders in 'RRCCBB':", os.listdir("rcb_dataset/RCB"))

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
from transformers import AutoTokenizer, AutoModel

text_data = torch.load("/content/text_features.pt")
print(f"Loaded {len(text_data)} samples.")
print(text_data[0])

# Text: BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")

# Image: ViT standard transforms
image_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
])

class BERTTextEncoder(nn.Module):
    def __init__(self, model_name="bert-base-multilingual-cased"):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)

    def forward(self, input_ids, attention_mask):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return output.last_hidden_state[:, 0, :]  # CLS token output (768-D)

import timm  # Make sure you installed this earlier

class ViTImageEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.vit = timm.create_model('vit_base_patch16_224', pretrained=True)
        self.vit.head = nn.Identity()  # remove classification layer

    def forward(self, x):
        return self.vit(x)  # Output: [batch_size, 768]

from torch.utils.data import Dataset

def load_image(path):
    return image_transform(Image.open(path).convert("RGB"))

def tokenize(text):
    tokens = tokenizer(text, padding="max_length", truncation=True, max_length=64, return_tensors="pt")
    return tokens["input_ids"].squeeze(0), tokens["attention_mask"].squeeze(0)

class MemeDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __getitem__(self, idx):
        sample = self.data[idx]
        input_ids, att_mask = tokenize(sample["cleaned_text"])
        image = load_image(sample["image_path"])
        label = torch.tensor(sample["label"], dtype=torch.float32)
        return input_ids, att_mask, image, label

    def __len__(self):
        return len(self.data)

from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

# Split 80% train, 20% test
train_data, test_data = train_test_split(text_data, test_size=0.2, random_state=42)

# Create Datasets
train_dataset = MemeDataset(train_data)
test_dataset = MemeDataset(test_data)

# Create Dataloaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

print(f"âœ… Loaded {len(train_dataset)} training samples and {len(test_dataset)} test samples")

class MultimodalClassifier(nn.Module):
    def __init__(self, text_dim=768, image_dim=768, fusion_dim=512):
        super().__init__()
        self.text_encoder = BERTTextEncoder()
        self.image_encoder = ViTImageEncoder()

        self.fusion = nn.Sequential(
            nn.LayerNorm(text_dim + image_dim),
            nn.Linear(text_dim + image_dim, fusion_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(fusion_dim, 1),
            nn.Sigmoid()
        )

    def forward(self, input_ids, attention_mask, images):
        text_feat = self.text_encoder(input_ids, attention_mask)  # [batch, 768]
        image_feat = self.image_encoder(images)                  # [batch, 768]
        combined = torch.cat([text_feat, image_feat], dim=1)      # [batch, 1536]
        out = self.fusion(combined)
        return out

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MultimodalClassifier().to(device)

criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)

for epoch in range(5):  # You can later increase to 10â€“15
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    for input_ids, att_mask, images, labels in train_loader:
        input_ids, att_mask = input_ids.to(device), att_mask.to(device)
        images, labels = images.to(device), labels.to(device).unsqueeze(1)

        optimizer.zero_grad()
        preds = model(input_ids, att_mask, images)
        loss = criterion(preds, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        predicted = (preds >= 0.5).float()
        correct += (predicted == labels).sum().item()
        total += labels.size(0)

    train_acc = correct / total
    print(f"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Train Accuracy: {train_acc:.4f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for input_ids, att_mask, images, labels in test_loader:
        input_ids, att_mask = input_ids.to(device), att_mask.to(device)
        images, labels = images.to(device), labels.to(device).unsqueeze(1)

        outputs = model(input_ids, att_mask, images)
        preds = (outputs >= 0.5).int()

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

# Compute Metrics
acc = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
cm = confusion_matrix(y_true, y_pred)

print("âœ… Test Evaluation")
print(f"Accuracy  : {acc:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1 Score  : {f1:.4f}")
print("Confusion Matrix:")
print(cm)

from PIL import Image
import torch
import pytesseract

def predict_image_percentages(image_path, model, tokenizer, image_transform, device):
    model.eval()

    # 1. Load and preprocess image
    image = Image.open(image_path).convert("RGB")
    image_tensor = image_transform(image).unsqueeze(0).to(device)

    # 2. Extract and clean text
    raw_text = pytesseract.image_to_string(image, lang='kan+eng')
    cleaned_text = re.sub(r'[^A-Za-z0-9à²…-à²¹à²¼à²½à²¾-à³Œà³ƒ-à³„à³¦-à³¯?!.,#*@ ]+', '', raw_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # 3. Tokenize text
    text_inputs = tokenizer(cleaned_text, padding="max_length", truncation=True,
                            max_length=64, return_tensors="pt")
    input_ids = text_inputs["input_ids"].to(device)
    att_mask = text_inputs["attention_mask"].to(device)

    # 4. Forward pass
    with torch.no_grad():
        logits = model(input_ids, att_mask, image_tensor)
        prob = torch.sigmoid(logits).item()

    offensive_pct = prob * 100
    non_offensive_pct = 100 - offensive_pct

    # 5. Output
    print("ğŸ–¼ï¸ Image:", image_path)
    print("ğŸ“ Cleaned Text:", cleaned_text)
    print(f"ğŸ¯ Offensive     : {offensive_pct:.2f}%")
    print(f"ğŸŸ¢ Non-Offensive: {non_offensive_pct:.2f}%")

    return offensive_pct, non_offensive_pct

# Just give image path here
import re
image_path = "/content/extracted/offensive/offensive/362.png"  # ğŸ” Replace with actual image

predict_image_percentages(image_path, model, tokenizer, image_transform, device)

# Just give image path here
import re
image_path = "/content/extracted/non_offensive/non_offensive/Screenshot 2025-03-23 at 8.08.25Î“Ã‡Â»PM.png"  # ğŸ” Replace with actual image

predict_image_percentages(image_path, model, tokenizer, image_transform, device)

import zipfile, os, pandas as pd

# === 1. Define ZIP paths
offensive_zip = "/content/offensive.zip"
non_offensive_zip = "/content/non_offensive.zip"

# === 2. Extraction directories
extract_root = "/content/extracted"
offensive_dir = os.path.join(extract_root, "offensive")
non_offensive_dir = os.path.join(extract_root, "non_offensive")

os.makedirs(offensive_dir, exist_ok=True)
os.makedirs(non_offensive_dir, exist_ok=True)

# === 3. Extract both zip files
with zipfile.ZipFile(offensive_zip, 'r') as zip_ref:
    zip_ref.extractall(offensive_dir)

with zipfile.ZipFile(non_offensive_zip, 'r') as zip_ref:
    zip_ref.extractall(non_offensive_dir)

# === 4. Collect image paths and remove duplicates
from pathlib import Path

def collect_images(folder_path, label):
    image_set = set()
    for root, _, files in os.walk(folder_path):
        for fname in files:
            if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.webp', '.bmp')):
                if not fname.startswith('.') and '__MACOSX' not in root:
                    full_path = os.path.abspath(os.path.join(root, fname))
                    image_set.add((full_path, label))
    return image_set

# === 5. Combine both sets and remove duplicates
offensive_images = collect_images(offensive_dir, 1)
non_offensive_images = collect_images(non_offensive_dir, 0)

all_images = list(offensive_images.union(non_offensive_images))

# === 6. Save to CSV
df = pd.DataFrame(all_images, columns=["image_path", "label"])
df.to_csv("image_labels.csv", index=False)

print(f"âœ… Saved {len(df)} unique image entries to 'image_labels.csv'")

import pandas as pd

# Load your human-labeled CSV
df = pd.read_csv("/content/image_labels.csv")

# Optional: check sample
print(df.head())
print(f"âœ… Loaded {len(df)} human-labeled images")

# Fix nested paths like /offensive/offensive/ or /non_offensive/non_offensive/
df["image_path"] = df["image_path"].str.replace(r"/(offensive|non_offensive)/\1/", r"/\1/", regex=True)

# Optional: verify a few samples
print(df["image_path"].head())

!pip install pytesseract
import torch
from PIL import Image
import pytesseract
import torchvision.transforms as T
import re
from transformers import AutoTokenizer

# Device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set model to eval
model.eval()

# ViT image transform (must match training)
vit_transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
    T.Normalize([0.5]*3, [0.5]*3)
])

# BERT tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")

# Clean function
def clean_text(text):
    cleaned = re.sub(r'[^A-Za-z0-9à²…-à²¹à²¼à²½à²¾-à³Œà³ƒ-à³„à³¦-à³¯?!.,#*@ ]+', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned

def predict_offensive_score(image_path):
    try:
        image = Image.open(image_path).convert("RGB")

        # OCR + clean
        raw_text = pytesseract.image_to_string(image, lang='kan+eng')
        text = clean_text(raw_text)

        # Tokenize text
        text_inputs = tokenizer(text, padding="max_length", truncation=True, max_length=64, return_tensors="pt")
        input_ids = text_inputs["input_ids"].to(device)
        att_mask = text_inputs["attention_mask"].to(device)

        # Image tensor
        image_tensor = vit_transform(image).unsqueeze(0).to(device)

        # Predict
        with torch.no_grad():
            logits = model(input_ids, att_mask, image_tensor)
            prob = float(torch.sigmoid(logits).item())  # convert logit to probability

        return prob

    except Exception as e:
        print(f"âŒ Error on {image_path}: {e}")
        return None

import os

# âœ… DO NOT REMOVE nested folders â€” your paths are accurate as is
# Just remove MACOSX system folders
df = df[~df["image_path"].str.contains("__MACOSX")]

# âœ… Filter to only valid image paths
df = df[df["image_path"].apply(os.path.exists)].reset_index(drop=True)

model_scores = []

for i, row in df.iterrows():
    prob = predict_offensive_score(row["image_path"])
    model_scores.append(prob)

df["model_score"] = model_scores

alpha = 0.7  # or whatever value you used for fusion

# Calculate the aggregated score (hybrid)
df["aggregated_score"] = alpha * df["model_score"] + (1 - alpha) * df["label"]

# Assign final binary label from the hybrid score
df["final_label"] = df["aggregated_score"].apply(lambda x: 1 if x >= 0.5 else 0)

df.to_csv("aggregated_model_labels.csv", index=False)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Compare final aggregated decision with human ground truth
y_true = df["label"]
y_pred = df["final_label"]

accuracy  = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, zero_division=0)
recall    = recall_score(y_true, y_pred, zero_division=0)
f1        = f1_score(y_true, y_pred, zero_division=0)
cm        = confusion_matrix(y_true, y_pred)

# ğŸ“¢ Print results
print("ğŸ“Š Aggregated Decision Evaluation")
print(f"âœ… Accuracy  : {accuracy:.4f}")
print(f"ğŸ¯ Precision : {precision:.4f}")
print(f"ğŸ” Recall    : {recall:.4f}")
print(f"ğŸ§® F1 Score  : {f1:.4f}")
print("\nğŸ§© Confusion Matrix:")
print(cm)

print(df["model_score"].isna().sum())
print(df["final_label"].value_counts(dropna=False))

# Model prediction
df["model_label"] = df["model_score"].apply(lambda x: 1 if x >= 0.5 else 0)

# Cases where model was wrong but aggregation was correct
saved_cases = df[(df["model_label"] != df["label"]) & (df["final_label"] == df["label"])]

print(f"âœ… Aggregation corrected {len(saved_cases)} model mistakes out of {len(df)} total samples.")

# Select the columns to export
cols = ["image_path", "label", "model_score", "model_label", "aggregated_score", "final_label"]

# Save to CSV
saved_cases[cols].to_csv("aggregation_corrected_cases.csv", index=False)
print("âœ… Saved corrected cases to 'aggregation_corrected_cases.csv'")

df["label"] = df["final_label"]

import pytesseract
from PIL import Image

def extract_text(path):
    try:
        image = Image.open(path).convert("RGB")
        return pytesseract.image_to_string(image, lang='kan+eng')
    except:
        return ""

df["text"] = df["image_path"].apply(extract_text)

"""Aggregated Model Training"""

# âœ… Colab Notebook: BERT + ViT Fusion Model for Meme Classification

# Step 1: Install required libraries
!pip install transformers timm pytesseract

# Step 2: Imports
import os
import pandas as pd
import torch
import torch.nn as nn
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from torchvision import transforms
from transformers import AutoTokenizer, AutoModel
import pytesseract
import timm

# Step 3: Load CSV and extract OCR text
csv_path = "/content/aggregated_model_labels.csv"
df = pd.read_csv(csv_path)
df = df[~df["image_path"].str.contains("__MACOSX")]
df = df[df["image_path"].apply(os.path.exists)].reset_index(drop=True)
df["label"] = df["final_label"]

if "text" not in df.columns:
    def extract_text(path):
        try:
            img = Image.open(path).convert("RGB")
            return pytesseract.image_to_string(img, lang='kan+eng')
        except:
            return ""
    df["text"] = df["image_path"].apply(extract_text)

# Step 4: Tokenizer and ViT transforms
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")
vit_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# Step 5: Dataset class
class MemeDataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row["image_path"]).convert("RGB")
        image_tensor = vit_transform(image)

        tokens = tokenizer(row["text"], padding="max_length", truncation=True, max_length=64, return_tensors="pt")
        input_ids = tokens["input_ids"].squeeze(0)
        attention_mask = tokens["attention_mask"].squeeze(0)

        label = torch.tensor(row["label"], dtype=torch.float32)
        return input_ids, attention_mask, image_tensor, label

# Step 6: Split & DataLoaders
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_loader = DataLoader(MemeDataset(train_df), batch_size=16, shuffle=True)
test_loader = DataLoader(MemeDataset(test_df), batch_size=16)

# Step 7: Fusion Model with ViT
class FusionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.text_encoder = AutoModel.from_pretrained("bert-base-multilingual-cased")
        self.image_encoder = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)
        self.fusion = nn.Sequential(
            nn.Linear(768 + 768, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 1)
        )

    def forward(self, input_ids, attention_mask, image):
        text_feat = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask).pooler_output  # [batch, 768]
        image_feat = self.image_encoder(image)  # [batch, 768]
        combined = torch.cat([text_feat, image_feat], dim=1)  # [batch, 1536]
        return self.fusion(combined)  # [batch, 1]

# Step 8: Train Loop
model = FusionModel().to("cuda" if torch.cuda.is_available() else "cpu")
device = next(model.parameters()).device
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

for epoch in range(5):
    model.train()
    total_loss, correct, total = 0, 0, 0
    for input_ids, att_mask, images, labels in train_loader:
        input_ids, att_mask = input_ids.to(device), att_mask.to(device)
        images, labels = images.to(device), labels.to(device).unsqueeze(1)

        optimizer.zero_grad()
        logits = model(input_ids, att_mask, images)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()

        preds = (torch.sigmoid(logits) >= 0.5).float()
        correct += (preds == labels).sum().item()
        total += labels.size(0)
        total_loss += loss.item()

    print(f"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Train Accuracy: {correct/total:.4f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# ğŸ§ª Evaluation Phase
model.eval()
y_true, y_pred = [], []

with torch.no_grad():
    for input_ids, att_mask, images, labels in test_loader:
        input_ids = input_ids.to(device)
        att_mask = att_mask.to(device)
        images = images.to(device)
        labels = labels.to(device)

        logits = model(input_ids, att_mask, images)
        probs = torch.sigmoid(logits)
        preds = (probs >= 0.5).float()

        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

# ğŸ§¾ Compute metrics
accuracy  = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, zero_division=0)
recall    = recall_score(y_true, y_pred, zero_division=0)
f1        = f1_score(y_true, y_pred, zero_division=0)
cm        = confusion_matrix(y_true, y_pred)

# ğŸ“Š Print results
print("\nğŸ“Š Final Test Evaluation")
print(f"âœ… Accuracy  : {accuracy:.4f}")
print(f"ğŸ¯ Precision : {precision:.4f}")
print(f"ğŸ” Recall    : {recall:.4f}")
print(f"ğŸ§® F1 Score  : {f1:.4f}")
print("\nğŸ§© Confusion Matrix:")
print(cm)

from PIL import Image
import torch
import pytesseract

def predict_image_percentages(image_path, model, tokenizer, image_transform, device):
    model.eval()

    # 1. Load and preprocess image
    image = Image.open(image_path).convert("RGB")
    image_tensor = image_transform(image).unsqueeze(0).to(device)

    # 2. Extract and clean text
    raw_text = pytesseract.image_to_string(image, lang='kan+eng')
    cleaned_text = re.sub(r'[^A-Za-z0-9à²…-à²¹à²¼à²½à²¾-à³Œà³ƒ-à³„à³¦-à³¯?!.,#*@ ]+', '', raw_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # 3. Tokenize text
    text_inputs = tokenizer(cleaned_text, padding="max_length", truncation=True,
                            max_length=64, return_tensors="pt")
    input_ids = text_inputs["input_ids"].to(device)
    att_mask = text_inputs["attention_mask"].to(device)

    # 4. Forward pass
    with torch.no_grad():
        logits = model(input_ids, att_mask, image_tensor)
        prob = torch.sigmoid(logits).item()

    offensive_pct = prob * 100
    non_offensive_pct = 100 - offensive_pct

    # 5. Output
    print("ğŸ–¼ï¸ Image:", image_path)
    print("ğŸ“ Cleaned Text:", cleaned_text)
    print(f"ğŸ¯ Offensive     : {offensive_pct:.2f}%")
    print(f"ğŸŸ¢ Non-Offensive: {non_offensive_pct:.2f}%")

    return offensive_pct, non_offensive_pct

# Just give image path here
import re
image_path = "/content/extracted/offensive/offensive/362.png"  # ğŸ” Replace with actual image

predict_image_percentages(image_path, model, tokenizer, image_transform, device)

# Just give image path here
import re
image_path = "/content/extracted/non_offensive/non_offensive/Screenshot 2025-03-23 at 8.08.25Î“Ã‡Â»PM.png"  # ğŸ” Replace with actual image

predict_image_percentages(image_path, model, tokenizer, image_transform, device)