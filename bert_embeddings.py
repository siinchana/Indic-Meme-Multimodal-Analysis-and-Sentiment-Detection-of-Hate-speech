# -*- coding: utf-8 -*-
"""BERT_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_jEpXIFavV0vEurbN9FBc0KRdxZiAs1I
"""

!pip install transformers

from transformers import AutoTokenizer, AutoModel
import torch.nn as nn
import torch

# Load cleaned text data (from LSTM preprocessing stage)
text_data = torch.load("/content/text_features.pt")

# Check one sample
print(text_data[0].keys())
# Should show: 'cleaned_text', 'label', 'image_path', ...

bert_model_name = "bert-base-multilingual-cased"  # Or try "ai4bharat/indic-bert"

tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
bert_model = AutoModel.from_pretrained(bert_model_name)
bert_model.eval()

def preprocess_bert_inputs(text_data, tokenizer, max_len=64):
    processed = []
    for sample in text_data:
        tokens = tokenizer(
            sample["cleaned_text"],
            padding="max_length",
            truncation=True,
            max_length=max_len,
            return_tensors="pt"
        )
        processed.append({
            "input_ids": tokens["input_ids"].squeeze(0),
            "attention_mask": tokens["attention_mask"].squeeze(0),
            "label": torch.tensor(sample["label"]),
            "image_path": sample["image_path"],
            "cleaned_text": sample["cleaned_text"]
        })
    return processed

bert_inputs = preprocess_bert_inputs(text_data, tokenizer)

class BERTTextEncoder(nn.Module):
    def __init__(self, model_name):
        super(BERTTextEncoder, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():
            output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
            cls_embedding = output.last_hidden_state[:, 0, :]  # [CLS] token output
            return cls_embedding  # Shape: [batch_size, 768]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_encoder = BERTTextEncoder(bert_model_name).to(device)
bert_encoder.eval()

bert_text_embeddings = []

with torch.no_grad():
    for sample in bert_inputs:
        input_ids = sample["input_ids"].unsqueeze(0).to(device)
        attention_mask = sample["attention_mask"].unsqueeze(0).to(device)
        embedding = bert_encoder(input_ids, attention_mask)  # [1, 768]

        # Save everything
        bert_text_embeddings.append({
            "embedding": embedding.squeeze(0).cpu(),  # 768-dim vector
            "label": sample["label"],
            "cleaned_text": sample["cleaned_text"],
            "image_path": sample["image_path"]
        })

torch.save(bert_text_embeddings, "text_bert_embeddings.pt")
print("âœ… Saved: text_bert_embeddings.pt")

from google.colab import files
files.download("text_bert_embeddings.pt")